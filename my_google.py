#!/usr/bin/env python3


import os
import random
import re
import string
import traceback
import urllib.parse
import sys
import concurrent.futures

from duckduckgo_search import DDGS
import googlesearch
import trafilatura

import gpt_basic
import my_groq
import cfg
import my_log


def download_text(urls: list, max_req: int = cfg.max_request, no_links = False) -> str:
    """
    Downloads text from a list of URLs and returns the concatenated result.
    
    Args:
        urls (list): A list of URLs from which to download text.
        max_req (int, optional): The maximum length of the result string. Defaults to cfg.max_request.
        no_links(bool, optional): Include links in the result. Defaults to False.
        
    Returns:
        str: The concatenated text downloaded from the URLs.
    """
    #max_req += 5000 # 5000 дополнительно под длинные ссылки с запасом
    result = ''
    newconfig = trafilatura.settings.use_config()
    newconfig.set("DEFAULT", "EXTRACTION_TIMEOUT", "0")
    for url in urls:
        content = trafilatura.fetch_url(url)
        # text = trafilatura.extract(content, config=newconfig, include_links=True, deduplicate=True, \
        #                            include_comments = True)
        text = trafilatura.extract(content, config=newconfig, deduplicate=True)
        if text:
            if no_links:
                result += f'\n\n{text}\n\n'
            else:
                result += f'\n\n|||{url}|||\n\n{text}\n\n'
            if len(result) > max_req:
                break
    return result


def download_text_new(urls: list, max_req: int = cfg.max_request) -> str:
    """
    Downloads text from a list of URLs and returns the concatenated result.

    Args:
        urls (list): A list of URLs from which to download text.
        max_req (int, optional): The maximum length of the result string. Defaults to cfg.max_request.

    Returns:
        str: The concatenated text downloaded from the URLs.
    """
    max_req += 10000  # 10000 дополнительно под длинные ссылки с запасом
    result = ''
    newconfig = trafilatura.settings.use_config()
    newconfig.set("DEFAULT", "EXTRACTION_TIMEOUT", "0")
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(fetch_and_extract, url, newconfig) for url in urls]

        for future in concurrent.futures.as_completed(futures):
            text = future.result()
            if text:
                result += text
                if len(result) > max_req:
                    break

    return result


def fetch_and_extract(url, newconfig):
    content = trafilatura.fetch_url(url)
    text = trafilatura.extract(content, config=newconfig, include_links=True, deduplicate=True, include_comments=True)
    if text:
        return f'\n\n|||{url}|||\n\n{text}\n\n'
    return ''


def ask_gpt(query: str, max_req: int, history: str, result: str, engine: str) -> str:
    """
	Ask GPT to respond to a user query using the results from a Google search on the query.
	Ignore any unclear characters in the search results as they should not affect the response.
	The response should only include what the user searched for and should not include anything they didn't search for.
	Try to understand the meaning of the user's query and what they want to see in the response.
	If it is not possible to answer such queries, then convert everything into a joke.

	Parameters:
	- query (str): The user's query.
	- max_req (int): The maximum number of characters to use from the query.
	- history (str): The previous conversation history.
	- result (str): The results from the Google search on the query.
    - engine (str): Google or DuckDuckGo.

	Return:
	- str: The response generated by GPT based on the given query and search results.
    """

    text = f"""Ответь на запрос юзера, используй результаты поиска в {engine} по этому запросу,
игнорируй непонятные символы в результатах поиска, они не должны влиять на ответ,
в ответе должно быть только то что юзер искал, и не должно быть того что не искал,
постарайся понять смысл его запроса и что он хочет увидеть в ответ,
если на такие запросы нельзя отвечать то переведи всё в шутку.


О чем говорили до этого: {history}


Запрос: {query}


Результаты поиска по этому запросу:


{result}"""

    result = gpt_basic.ai(text[:max_req], max_tok=cfg.max_google_answer)
    my_log.log_google(text[:max_req], result)
    return result


def ask_gpt2(query: str, max_req: int, history: str, result: str, engine: str) -> str:
    """
    Спрашивает у chatGPT есть ли хороший ответ на запрос юзера в найденной странице.
    Если есть то возвращает ответ, иначе пустую строку.
    Parameters:
        - query (str): The query string.
        - max_req (int): The maximum number of requests.
        - history (str): The history string.
        - result (str): The result string.
        - engine (str): The engine string.

    Returns:
        - bool: True/False
    """
    text = f"""Юзер ищет информацию, тебе надо понять, есть ли хороший ответ на его запрос в найденной веб странице,
постарайся понять смысл его запроса и что он хочет увидеть в ответе,
В ответе должно быть только одно слово, True если есть и False во всех остальных случаях.
    
О чем говорили до этого: {history}

Запрос юзера: {query}

Веб страница и ее содержимое:
    
{result}
"""
    try:
        result2 = gpt_basic.ai(text[:max_req], max_tok=cfg.max_google_answer).lower()
        
        if 'true' in result2:
            result3 = ask_gpt3(query, max_req, history, result, engine)
            return result3
        else:
            return ''
    except Exception as error:
        print(error)
        return ''


def ask_gpt3(query: str, max_req: int, history: str, result: str, engine: str) -> str:
    """
    Спрашивает у chatGPT ответ на запрос юзера на основе содержания веб страницы.
	Parameters:
	- query (str): The user's query.
	- max_req (int): The maximum number of characters to use from the query.
	- history (str): The previous conversation history.
	- result (str): The results from the Google search on the query.
    - engine (str): Google or DuckDuckGo.

	Return:
	- str: The response generated by GPT based on the given query and search results.
    """

    text = f"""Ответь на запрос юзера, используй текст веб страницы,
в ответе должно быть только то что юзер искал, и не должно быть того что не искал,
постарайся понять смысл его запроса и что он хочет увидеть в ответ.


О чем говорили до этого: {history}


Запрос юзера: {query}


Текст страницы:


{result}"""

    result = gpt_basic.ai(text[:max_req], max_tok=cfg.max_google_answer)
    my_log.log_google(text[:max_req], result)
    return result


def search_google(query: str, max_req: int = cfg.max_request, max_search: int = 20, history: str = '') -> str:
    """ищет в гугле ответ на вопрос query, отвечает с помощью GPT
    max_req - максимальный размер ответа гугла, сколько текста можно отправить гпт чату
    max_search - сколько ссылок можно прочитать пока не наберется достаточно текстов
    history - история диалога, о чем говорили до этого
    """

    max_req = max_req - len(history)
    # добавляем в список выдачу самого гугла, и она же первая и главная
    urls = [f'https://www.google.com/search?q={urllib.parse.quote(query)}',]
    # добавляем еще несколько ссылок, возможно что внутри будут пустышки, джаваскрипт заглушки итп
    r = googlesearch.search(query, stop = max_search)
    bad_results = ('https://g.co/','.pdf','.docx','.xlsx', '.doc', '.xls')
    for url in r:
        if any(s.lower() in url.lower() for s in bad_results):
            continue
        urls.append(url)
    result = download_text(urls, max_req)

    # text, links = shorten_links(result)
    # answer = ask_gpt(query, max_req, history, text, 'Google')
    # return restore_links(answer, links)
    text = result
    answer = ask_gpt(query, max_req, history, text, 'Google')
    return answer


def search_google_iter(query: str, max_req: int = cfg.max_request, max_search: int = 10, history: str = '') -> str:
    """ищет в гугле ответ на вопрос query, отвечает с помощью GPT
    ищет по одному сайту за раз
    max_req - максимальный размер ответа гугла, сколько текста можно отправить гпт чату
    max_search - сколько ссылок можно прочитать пока не наберется достаточно текстов
    history - история диалога, о чем говорили до этого
    """

    max_req = max_req - len(history)
    # # добавляем в список выдачу самого гугла, и она же первая и главная
    # urls = [f'https://www.google.com/search?q={urllib.parse.quote(query)}',]
    
    urls = []
    
    # добавляем еще несколько ссылок, возможно что внутри будут пустышки, джаваскрипт заглушки итп
    r = googlesearch.search(query, stop = max_search, \
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36')
    bad_results = ('https://g.co/','.pdf','.docx','.xlsx', '.doc', '.xls')
    for url in r:
        if any(s.lower() in url.lower() for s in bad_results):
            continue
        urls.append(url)

    for url in urls:
        result = download_text((url,), max_req)
        if result:
            answer = ask_gpt2(query, max_req, history, result, 'Google')
            if answer:
                return answer

    return 'Не удалось ничего найти.'


def ddg_text(query: str) -> str:
    """
    Generate a list of URLs from DuckDuckGo search results based on the given query.

    Parameters:
        query (str): The search query.

    Returns:
        str: A URL from each search result.
    """
    with DDGS() as ddgs:
        for result in ddgs.text(query, safesearch='Off', timelimit='y', region = 'ru-ru'):
            yield result['href']


def search_ddg(query: str, max_req: int = cfg.max_request, max_search: int = 10, history: str = '') -> str:
    """ищет в ddg ответ на вопрос query, отвечает с помощью GPT
    max_req - максимальный размер ответа гугла, сколько текста можно отправить гпт чату
    max_search - сколько ссылок можно прочитать пока не наберется достаточно текстов
    history - история диалога, о чем говорили до этого
    """
    max_req = max_req - len(history)
    urls = []
    # добавляем еще несколько ссылок, возможно что внутри будут пустышки, джаваскрипт заглушки итп
    bad_results = ('https://g.co/','.pdf','.docx','.xlsx', '.doc', '.xls')
    for url in ddg_text(query):
        if any(s.lower() in url.lower() for s in bad_results):
            continue
        urls.append(url)
    result = download_text(urls, max_req)
    
    # text, links = shorten_links(result)
    # answer = ask_gpt(query, max_req, history, text, 'DuckDuckGo')
    # return restore_links(answer, links)
    text = result
    answer = ask_gpt(query, max_req, history, text, 'DuckDuckGo')
    return answer


def search_ddg_iter(query: str, max_req: int = cfg.max_request, max_search: int = 10, history: str = '') -> str:
    """ищет в ddg ответ на вопрос query, отвечает с помощью GPT
    Скачивает последовательно по одному сайту и спрашивает у chatGPT есть ли тут ответ или надо продолжить поиски
    max_req - максимальный размер ответа гугла, сколько текста можно отправить гпт чату
    max_search - сколько ссылок можно прочитать пока не наберется достаточно текстов
    history - история диалога, о чем говорили до этого
    """
    max_req = max_req - len(history)
    urls = []
    # добавляем еще несколько ссылок, возможно что внутри будут пустышки, джаваскрипт заглушки итп
    bad_results = ('https://g.co/','.pdf','.docx','.xlsx', '.doc', '.xls')
    for url in ddg_text(query):
        if any(s.lower() in url.lower() for s in bad_results):
            continue
        urls.append(url)
    
    for url in urls:
        result = download_text((url,), max_req)
        if result:
            answer = ask_gpt2(query, max_req, history, result, 'DuckDuckGo')
            if answer:
                return answer

    return 'Не удалось ничего найти.'


def search(query: str) -> str:
    """
    Search for a query string using Google search and return the result.
    Search for a query string using DuckDuckGo if Google fails.

    Parameters:
        query (str): The query string to search for.

    Returns:
        str: The search result.
    """
    try:
        result = search_google(query)
    except urllib.error.HTTPError as error:
        if 'HTTP Error 429: Too Many Requests' in str(error):
            result = search_ddg(query)
            my_log.log2(f'my_google:search: query')
        else:
            print(error)
            raise error
    return result


def shorten_link(link: str) -> str:
    """возвращает сокращенную ссылку, https://somesite.com/long_link -> https://sh.rt/random6symbols
    если ссылка длиннее чем 60 символов иначе вернет ''"""
    if len(link) < 60:
        return ''
    base_url = 'https://sh.rt/'
    random_chars = ''.join(random.choices(string.ascii_letters + string.digits, k=6))
    short_link = base_url + random_chars
    return short_link


def shorten_links(text: str) -> list:
    """сокращает ссылки в тексте, возвращает текст с замененными ссылками и словарь для обратной замены"""
    links = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
    replace_links = {}
    for link in links:
        short = shorten_link(link)
        if short:
            replace_links[link] = short
            text = text.replace(link, short)
    return text, replace_links


def restore_links(text: str, replace_links: dict) -> str:
    """восстанавливает полные ссылки в тексте, используя словарь замен"""
    for link, short in replace_links.items():
        text = text.replace(short, link)
    return text



def search_v4(query: str, lang: str = 'ru', max_search: int = 10) -> str:
    # добавляем в список выдачу самого гугла, и она же первая и главная
    urls = [f'https://www.google.com/search?q={urllib.parse.quote(query)}',]
    # добавляем еще несколько ссылок, возможно что внутри будут пустышки, джаваскрипт заглушки итп
    try:
        r = googlesearch.search(query, stop = max_search, lang=lang)
    except Exception as error:
        my_log.log2(f'my_google:search_google_v2: {error}')
        try:
            r = [x for x in ddg_text(query)]
        except Exception as error:
            my_log.log2(f'my_google:search_google_v2: {error}')
            return ''

    bad_results = ('https://g.co/','.pdf','.docx','.xlsx', '.doc', '.xls')

    try:
        for url in r:
            if any(s.lower() in url.lower() for s in bad_results):
                continue
            urls.append(url)
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log2(f'my_google:search_v3: {error}\n\n{error_traceback}')

    text = ''
    for url in urls:
        # print(url)
        text += download_text([url,], 5000)
        if len(text) > 12000:
            break

    q = f'''Answer in "{lang}" language to users search query using search results and your own knowledge.
User query: "{query}"

Search results:

{text[:12000]}
'''
    return my_groq.ai(q, max_tokens_ = 4000)


if __name__ == "__main__":
    print(search_v4('курс доллара'))
    print(search_v4('3 закона робототехники'))
